\documentclass[10pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage[ngerman]{babel}
\usepackage[automark]{scrpage2}
\usepackage{amsmath,amssymb,amstext}
%\usepackage{mathtools}
\usepackage[]{color}
\usepackage[]{enumerate}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage[perpage,para,symbol*]{footmisc}
\usepackage{listings} 
%\usepackage[pdfborder={0 0 0},colorlinks=false]{hyperref}
\usepackage{natbib}     % Neuimplementierung des \cite-Kommandos
\usepackage{bibgerm}      % Deutsche Bezeichnungen
\usepackage{color}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{a4wide}
\usepackage{xspace}
\usepackage{listings}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage[colorinlistoftodos,textsize=small,textwidth=2cm,shadow,bordercolor=black,backgroundcolor={red!100!green!33},linecolor=black]{todonotes}%disable
\usepackage{nameref}
\usepackage{titleref}
 
\usepackage[colorlinks,
	pdftex,	
	bookmarks,
	bookmarksopen=false,
	bookmarksnumbered,
	citecolor=blue,
	linkcolor=black, %orig: blue
	urlcolor=blue,
	filecolor=blue,
	linktocpage,
    pdfstartview=Fit,                                  % startet mit Ganzseitenanzeige    
	pdfauthor={Pahl, Roetting, Jaeger, Steenbuck, Muenchow, Noetzel, Steudte}]{hyperref}
    


\usepackage[all]{hypcap}
\hypersetup{pdfsubject={TT2 Problemstellung 3 : Ausarbeitung}, pdftitle={Reinforcement Learning}, pdfkeywords={Framework, Monte Carlo Methode, Dynamic Programming, Temporal Difference Learning, Reinforcement Learning}}



\lstset{numbers=left, numberstyle=\tiny, numbersep=5pt, breaklines=true, showstringspaces=false} 

%changehere
\def\titletext{TT2 Problemstellung 3 : Ausarbeitung}
\def\titletextshort{Problemstellung 3}
\author{Pascal Jäger, Stefan Münchow, Armin Steudte,\\ Milena Rötting, Svend-Anjes Pahl, Carsten Noetzel, Oliver Steenbuck}

\title{\titletext}

%changehere Datum der Übung
\date{01.06.2012}

\pagestyle{scrheadings}
%changehere
\ihead{TT2, Neitzke}
\ifoot{Generiert am:\\ \today}

\cfoot{Pascal Jäger, Stefan Münchow,\\ Armin Steudte, Milena Rötting,\\ Svend-Anjes Pahl, Carsten Noetzel, Oliver Steenbuck}


\ohead[]{\titletextshort}
\ofoot[]{{\thepage} / \pageref{LastPage}}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}

\begin{document}
\maketitle

\setcounter{tocdepth}{3}
\tableofcontents
\listoffigures
%\lstlistoflistings

\section{Fragestellung 1: V-Werte / Q-Werte}
Die Value-Function $V(s)$ definiert die mittlere Belohnung, wenn man einer Strategie $\pi$ ab dem Zustand $s$ folgt. Um die mittlere Belohnung zu bestimmen, ist es notwendig entweder eine Stategie $\pi$ vorgegeben zu haben, welche bewertet werden soll (Policy Evaluation), oder es muss das Modell der Umwelt, das heißt die Zustandsübergangsfunktion $\delta$ und die Belohnungsfunktion $r$, bekannt sein. Ist das Modell der Umwelt bekannt, so kann mit Hilfe von beispielsweise der Value-Iteration die optimale Strategie $\pi^*$ ermittelt werden (Policy Improvement).

Besitzt man ein Modell der Umwelt, so ist es möglich mit Hilfe der Value-Iteration die optimale Policy $\pi^*$ iterativ zu bestimmen. Hierbei wird für die Bewertung jedes Zustandes, rekursiv auf die Bewertung der Nachbarzustände zugegriffen. Währenddessen wird die Zustandsübergangsfunktion $\delta$, sowie die Belohnungsfunktion $r$ sehr häufig aufgerufen, was ein Modell der Umgebung erforderlich macht.

Möchte man eine Strategie verbessern und besitzt kein Modell der Umwelt, so reicht die Bewertung einzelner Zustände nicht aus. Die Zustandsbewertung gibt keine Auskunft darüber, die Wahl welcher Aktion zur Zustandsbewertung geführt hat. Man besitzt zwar die aktuelle Strategie $\pi$, diese wird jedoch kontinuierlich verbessert. Damit ist kein Rückschluss einer Aktion auf eine Zustandsbewertung möglich.

Aus diesem Grund wird nicht nur die Bewertung des Zustands für die aktuelle Aktion der Strategie gespeichert, sondern es wird die Bewertung jeder Aktion für einen Zustand in einer Action-State-Funktion gespeichert. Auf diese Weise ist es möglich, die Strategie $\pi$ zu verändern und das Wissen über die Bewertung einzelner Aktionen zu behalten. Durch die Speicherung der Bewertung zu jeder einzelnen Aktion vervielfacht sich der Speicheraufwand.

\section{Fragestellung 2: SARSA / Q-Learning bei kleinem $\epsilon$}
\subsection{On-Policy: SARSA}
Basis für den SARSA-Algorithmus sind die Q-Werte, also Zustands-Aktions-Paare $Q(s,a)$. Die Bewertung eines solchen Zustands-Aktions-Paares erfolgt durch die unten genannte Gleichung welche auch in Abbildung \ref{fig:Sarsa} zu finden ist.\\
Der erwartete Wert $Q(s,a)$ für eine Aktion $a$ im Zustand $s$ wird dabei aktualisiert durch, den bereits ermittelten, erwarteten Wert $Q(s,a)$ und dem Schrittfaktor $\alpha$, der mit der Summe aus Reward für den Folgezustand $r_{t+1}$ und der Differenz aus dem discounted Reward für das Folge-Zustands-Aktions-Paar $\gamma Q(s_{t+1}, a_{t+1})$ und dem erwarteten Wert $Q(s,a)$ addiert wird.\\

\begin{equation}
Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha [r_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})]
\end{equation}

In Abbildung \ref{fig:Sarsa} ist zu erkennen, dass die Aktionen $a'$ im Zustand $s'$ in Abhängigkeit von der Policy gewählt werden, die zum Beispiel $\epsilon$-Greedy sein kann. Das heißt es wird die meiste Zeit diejenige Aktion $a'$ ausgewählt die den größten erwarteten Gewinn verspricht und mit einer Wahrscheinlichkeit $\epsilon$ eine zufällige Aktionen, um die Umgebung weiter zu explorieren und nicht deterministisch zu sein.

\begin{figure}[htbp]
	\centering	\includegraphics[width=0.8\textwidth]{Bilder/Sarsa.png}
	\caption{Sarsa: On Policy TD Control Algorithmus}
	\label{fig:Sarsa}
\end{figure}


\subsection{Off-Policy: Q-Learning}
Auch das Q-Learning Verfahren setzt auf den Q-Werten auf und berechnet diese über die unten genannte Formel, die Teil des Algorithmus in Abbildung \ref{fig:QLearning} ist.

\begin{equation}
Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha [r_{t+1} + \gamma max_{a} Q(s_{t+1},a) - Q(s_{t},a_{t})]
\end{equation} 

Der Unterschied zum Sarsa-Algorithmus ist hierbei der Teil $\gamma max_{a} Q(s_{t+1},a)$. Anstelle den erwarteten Return $Q(s_{t+1},a_{t+1})$ für die gewählte Aktion $a'$ im Zustand $s'$ mit dem Discount-Faktor $\gamma$ zu multiplizieren wie es beim Sarsa-Algorithmus gemacht wird, wird hier diejenige Aktion bestimmt, die den größten Reward verspricht und deren Reward $max_{a} Q(s_{t+1},a)$ mit dem Discount-Faktor multipliziert. \textbf{Damit wird der Q-Wert $Q(s,a)$ unabhängig von der Strategie, allein auf Basis des Returns und der Q-Werte benachbarter Zustände aktualisiert!}. 
Unhabhängig davon, welche Aktion die $\epsilon$-Greedy Policy wählt, wird immer der Q-Wert zur Berechnung genommen, die den höchsten Reward verspricht und nicht der, des durch die Policy gewählten Nachbarn. 

\begin{figure}[htbp]
	\centering	\includegraphics[width=0.8\textwidth]{Bilder/Q-Learning.png}
	\caption{Q Learning: Off Policy TD Control Algorithmus}
	\label{fig:QLearning}
\end{figure}


\subsection{Verhalten bei kleinem $\epsilon$}
Das Verhalten vom Sarsa-Algorithmus nähert sich dem vom Q-Learning an, da mit kleiner werdendem $\epsilon$ die Wahrscheinlichkeit sinkt eine zufällige Aktion auszuwählen. Bei sehr kleinem $\epsilon$ wird vermehrt die Aktion $a'$ ausgewählt, die den größten Reward verspricht, ähnlich wie es beim Q-Learning mit $max_{a} Q(s_{t+1},a)$ getan wird. Damit nährt sich das Verhalten von Sarsa dem des Q-Learnings an.

\section{Fragestellung 3: kontinuierliche Zustandsvariablen}
Für den Fall, dass die Zustandsvariablen kontinuierlich sind werden sie diskretisiert. Sollte der Zustandsraum durch die Diskretisierung zu groß werden, gibt es verschiedene Möglichkeiten, um den Zustandsraum zu verkleinern bzw. handhabbar zu machen.
\subsection{Partitionierung}
Bei der Partitionierung wird ein großer Zustandsraum in mehrere kleine Räume aufgeteilt. Ein Beispiel für die Partitionierung des Zustandsraumes ist Schach, bei dem der Zustandsraum in Eröffnung, Mittel- und Endspiel unterteilt werden kann. 

\subsection{Abstraktion}
Ein Beispiel für die Abstraktion ist Kniffel, in dem bspw. die Reihenfolge in der die Würfel geworfen wurden ignoriert wird und sie immer der Größe nach sortiert werden, um den Zustandsraum zu verkleinern. In diesem Fall werden dadurch nicht einmal Informationen verloren.

\subsection{Neuronale Netze}
Die Tabelle der Q- oder V-Werte kann durch ein neuronales Netz ersetzt werden, welches als Input die Variablen $a$ und $s$ erhält und als Ziel-Output den Q-Wert besitzt. Für jede Aktualisierung des Q-Werts gibt es ein Trainingsbeispiel mit genannten Inputs und Outputs. Durch das neuronale Netz kann die Funktion $Q(s,a)$, welche für unendlich viele Eingaben definiert ist, auf einen endlichen Raum reduziert werden.
Statt eines neuronalen Netzes ist es auch möglich, andere überwachte Lernverfahren, wie beispielsweise Support Vector Machines zu verwenden, um den unendlichen Zustandsraum zu generalisieren und somit zu begrenzen.


\section{Fragestellung 4: Kniffel}
Das Spiel Kniffel besitzt auf Grund der umfangreichen Spielregeln und Möglichkeiten, Werte auf dem Spielblock einzutragen, einen sehr großen Zustandsraum. Je nach Kombination und Reihenfolge der in den Spielbock eingetragenen Werte ist das Gesamtresultat des Spiels unterschiedlich. 

Für einen Kniffel-Agenten wird also ein Lernverfahren benötigt, welches die Spielsituation nach jedem Zug und nicht erst am Ende des Spiels betrachtet und bewertet.
Aus diesem Grund fallen die Algorithmen des Temporal Difference Learning (Q-Learning und SARSA) bereits heraus, da diese nur den direkt betroffenen Zustand und seine Aktion bewerten. Im Gegensatz dazu ist ein Monte Carlo-Algorithmus geeigneter, da diese Verfahren am Ende einer Episode jeden Schritt derselben betrachten und bewerten.

Auf Grund des großen Zustandsraumes eignen sich auch die Verfahren der dynamischen Programmierung nicht als Algorithmus für einen Agenten. Policy- und Value-Iteration berechnen die Erwartungswerte über den kompletten Zustandsraum. Dieser ist in dieser Problemstellung zu groß als dass er durchgerechnet werden kann.

Generell lässt sich für diese Problemstellung aber sagen, dass die Speicherung des kompletten Zustandsraumes schwierig ist. Beim Monte Carlo Verfahren müssen für alle auftretenden Zustands-Aktions-Paare Listen von Rewards für alle Episoden gespeichert werden, sodass diese in das weitere Lernen für die Durchschnittsbildung und die Verfeinerung der Erwartungswerte einfließen können. 

Auf Grund der Problembeschaffenheit müssten alle Verfahren zusätzlichen Aufwand betreiben, um den Zustandsraum klein zu bekommen.
\end{document}

