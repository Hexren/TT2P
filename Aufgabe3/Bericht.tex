\documentclass[10pt]{scrartcl}

\usepackage[utf8]{inputenc}
\usepackage{tabularx}
\usepackage[ngerman]{babel}
\usepackage[automark]{scrpage2}
\usepackage{amsmath,amssymb,amstext}
%\usepackage{mathtools}
\usepackage[]{color}
\usepackage[]{enumerate}
\usepackage{graphicx}
\usepackage{lastpage}
\usepackage[perpage,para,symbol*]{footmisc}
\usepackage{listings} 
%\usepackage[pdfborder={0 0 0},colorlinks=false]{hyperref}
\usepackage{natbib}     % Neuimplementierung des \cite-Kommandos
\usepackage{bibgerm}      % Deutsche Bezeichnungen
\usepackage{color}
\usepackage{colortbl}
\usepackage{listings}
\usepackage{a4wide}
\usepackage{xspace}
\usepackage{listings}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage[colorinlistoftodos,textsize=small,textwidth=2cm,shadow,bordercolor=black,backgroundcolor={red!100!green!33},linecolor=black]{todonotes}%disable
\usepackage{nameref}
\usepackage{titleref}
 
\usepackage[colorlinks,
	pdftex,	
	bookmarks,
	bookmarksopen=false,
	bookmarksnumbered,
	citecolor=blue,
	linkcolor=black, %orig: blue
	urlcolor=blue,
	filecolor=blue,
	linktocpage,
    pdfstartview=Fit,                                  % startet mit Ganzseitenanzeige    
	pdfauthor={Pahl, Roetting, Jaeger, Steenbuck, Muenchow, Noetzel, Steudte}]{hyperref}
    


\usepackage[all]{hypcap}
\hypersetup{pdfsubject={TT2 Problemstellung 3 : Ausarbeitung}, pdftitle={Reinforcement Learning}, pdfkeywords={Framework, Monte Carlo Methode, Dynamic Programming, Temporal Difference Learning, Reinforcement Learning}}



\lstset{numbers=left, numberstyle=\tiny, numbersep=5pt, breaklines=true, showstringspaces=false} 

%changehere
\def\titletext{TT2 Problemstellung 3 : Ausarbeitung}
\def\titletextshort{Problemstellung 3}
\author{Pascal Jäger, Stefan Münchow, Armin Steudte,\\ Milena Rötting, Sven-Andjes Pahl, Carsten Noetzel, Oliver Steenbuck}

\title{\titletext}

%changehere Datum der Übung
\date{01.06.2012}

\pagestyle{scrheadings}
%changehere
\ihead{TT2, Neitzke}
\ifoot{Generiert am:\\ \today}

\cfoot{Pascal Jäger, Stefan Münchow,\\ Armin Steudte, Milena Rötting,\\ Sven-Andjes Pahl, Carsten Noetzel, Oliver Steenbuck}


\ohead[]{\titletextshort}
\ofoot[]{{\thepage} / \pageref{LastPage}}

\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.1in}

\begin{document}
\maketitle

\setcounter{tocdepth}{3}
\tableofcontents
\listoffigures
%\lstlistoflistings

\section{Fragestellung 1: V-Werte / Q-Werte}

\section{Fragestellung 2: SARSA / Q-Learning bei kleinem $\epsilon$}
\subsection{On-Policy: SARSA}
Basis für den SARSA-Algorithmus sind die Q-Werte, also Zustands-Aktions-Paare $Q(s,a)$. Die Bewertung eines solchen Zustands-Aktions-Paares erfolgt durch die unten genannte Gleichung welche auch in Abbildung \ref{fig:Sarsa} zu finden ist.\\
Der erwartete Wert $Q(s,a)$ für eine Aktion $a$ im Zustand $s$ wird dabei aktualisiert durch, den bereits ermittelten, erwarteten Wert $Q(s,a)$ und dem Schrittfaktor $\alpha$, der mit der Summe aus Reward für den Folgezustand $r_{t+1}$ und der Differenz aus dem discounted Reward für das Folge-Zustands-Aktions-Paar $\gamma Q(s_{t+1}, a_{t+1})$ und dem erwarteten Wert $Q(s,a)$ addiert wird.\\

\begin{equation}
Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha [r_{t+1} + \gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})]
\end{equation}

In Abbildung \ref{fig:Sarsa} ist zu erkennen, dass die Aktionen $a'$ im Zustand $s'$ in Abhängigkeit von der Policy gewählt werden, die zum Beispiel $\epsilon$-Greedy sein kann. Das heißt es wird die meiste Zeit diejenige Aktion $a'$ ausgewählt die den größten erwarteten Gewinn verspricht und mit einer Wahrscheinlichkeit $\epsilon$ eine zufällige Aktionen, um die Umgebung weiter zu explorieren und nicht deterministisch zu sein.

\begin{figure}[htbp]
	\centering	\includegraphics[width=0.8\textwidth]{Bilder/Sarsa.png}
	\caption{Sarsa: On Policy TD Control Algorithmus}
	\label{fig:Sarsa}
\end{figure}


\subsection{Off-Policy: Q-Learning}
Auch das Q-Learning Verfahren setzt auf den Q-Werten auf und berechnet diese über die unten genannte Formel, die Teil des Algorithmus in Abbildung \ref{fig:QLearning} ist.

\begin{equation}
Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha [r_{t+1} + \gamma max_{a} Q(s_{t+1},a) - Q(s_{t},a_{t})]
\end{equation} 

Der Unterschied zum Sarsa-Algorithmus ist hierbei der Teil $\gamma max_{a} Q(s_{t+1},a)$. Anstelle den erwarteten Return $Q(s_{t+1},a_{t+1})$ für die gewählte Aktion $a'$ im Zustand $s'$ mit dem Discount-Faktor $\gamma$ zu multiplizieren wie es beim Sarsa-Algorithmus gemacht wird, wird hier diejenige Aktion bestimmt, die den größten Reward verspricht und deren Reward $max_{a} Q(s_{t+1},a)$ mit dem Discount-Faktor multipliziert. \textbf{Damit wird der Q-Wert $Q(s,a)$ unabhängig von der Strategie, allein auf Basis des Returns und der Q-Werte benachbarter Zustände aktualisiert!}. 
Unhabhängig davon, welche Aktion die $\epsilon$-Greedy Policy wählt, wird immer der Q-Wert zur Berechnung genommen, die den höchsten Reward verspricht und nicht der, des durch die Policy gewählten Nachbarn. 

\begin{figure}[htbp]
	\centering	\includegraphics[width=0.8\textwidth]{Bilder/Q-Learning.png}
	\caption{Q Learning: Off Policy TD Control Algorithmus}
	\label{fig:QLearning}
\end{figure}


\subsection{Verhalten bei kleinem $\epsilon$}
Das Verhalten vom Sarsa-Algorithmus nähert sich dem vom Q-Learning an, da mit kleiner werdendem $\epsilon$ die Wahrscheinlichkeit sinkt eine zufällige Aktion auszuwählen. Bei sehr kleinem $\epsilon$ wird vermehrt die Aktion $a'$ ausgewählt, die den größten Reward verspricht, ähnlich wie es beim Q-Learning mit $max_{a} Q(s_{t+1},a)$ getan wird. Damit nährt sich das Verhalten von Sarsa dem des Q-Learnings an.

\section{Fragestellung 3: kontinuierliche Zustandsvariablen}
Für den Fall, dass die Zustandsvariablen kontinuierlich sind werden sie diskretisiert. Sollte der Zustandsraum durch die Diskretisierung zu groß werden, findet entweder eine Partitionierung des Zustandsraumes oder eine Abstraktion von Zuständen statt. Das heißt ein großer Zustandsraum kann in mehrere kleine Räume aufgeteilt werden, oder es wird über die Zustände selbst abstrahiert, d.h. mehrere Zustände werden zu einem zusammengefasst.

Ein Beispiel für die Partitionierung des Zustandsraumes ist Schach, bei dem der Zustandsraum in Eröffnung, Mittel- und Endspiel unterteilt werden kann. Ein Beispiel für die Abstraktion ist Kniffel, in dem bspw. die Sortierung der Würfel ignoriert wird um den Zustandsraum zu verkleinern. In diesem Fall werden dadurch nicht einmal Informationen verloren.



\section{Fragestellung 4: Kniffel}

\todo{siehe den Link}
\url{http://www.haw-hamburg.de/fileadmin/user_upload/SchulCampus/Downloads/Arbeitskreis_Informatik/Kniffel_Agenten.pdf}
\listoftodos

\end{document}

